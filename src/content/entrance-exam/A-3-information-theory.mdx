---
title: 情報理論まとめ
created: 2025-07-31
tags:
- information-theory
- guide
modified: 2025-08-01
---

# エントロピーの定義

確率変数 $X$ に対して、

$$
H(X) = -\sum_{i}P_i \log_{2}{P_i}
$$

だいたい、情報源が発する符号は0と1の2種類なので、$-p_0\log_{2}{p_0}-p_1\log_{2}{p_1}$ みたいになる。

## 結合エントロピー

$$
H(X,Y) = -\sum_{i=0}^{M_X}\sum_{j=0}^{M_Y} p(x_i,y_j)\log_2{p(x_i,y_j)}
$$

## 条件付きエントロピー

$$
H(X|Y) = -\sum_{i=0}^{M_Y}p(y_i)\sum_{j=0}^{M_X}p(x_j|y_i)\log_2{p(x_j|y_i)}
$$

結合エントロピーとの関係は、

$$
H(X,Y) = H(Y) + H(X|Y)
$$

基本的に $H(X)$ より小さくなるはず。

### 状態遷移がある場合

状態集合を $\{s_a, s_b\}$ とする。

$p_{s_a|s_a}, p_{s_b|s_a}, p_{s_a|s_b}, p_{s_b|s_b}$ が状態遷移図を使うとわかる。
これのエントロピーを求めようと思うと、定常分布 $w_{s_a}, w_{s_b}$ がいる。
結局、

$$
-w_{s_a}(p_{s_a|s_a}\log_2 p_{s_a|s_a} + p_{s_b|s_a}\log_2 p_{s_b|s_a}) - w_{s_b}(p_{s_a|s_b}\log_2 p_{s_a|s_b}+p_{s_b|s_b}\log_2 p_{s_b|s_b})
$$

みたいな式になる。

## 相互情報量

Yを知ることで、Xの曖昧さがどのぐらい減ったかをそのエントロピーの差で計算できる。

$$
I(X;Y) = H(X) - H(X|Y)
$$

この値は必ず0以上になるので、負になった場合はどこかで計算を間違えている。

# 通信路符号化

## 情報速度

$r$ 元符号の符号語の数を $M$ としたとき、符号長を $n$ とすると、

$$
R = \frac{\log_2 M}{n}
$$

## 通信路容量

情報速度の限界値のこと。

$$
C = \lim_{n\to \infty} \max_{P_{X_n}\in\mathcal{P}_n} I(X_n;Y_n)/n
$$

## いろいろな通信路

### 記憶のない定常通信路

通信路容量は

$$
C=\max_{P_X\in\mathcal{P}} I(X;Y)
$$

### 記憶のない2元対称通信路 (BSC)

ビット誤り率 $p$ のとき、通信路行列は以下。
1行目に 0 to 0 の確率、0 to 1 の確率を書いていて、2行目は 1 to 0 の確率、1 to 1 の確率が書かれている。

$$
\begin{pmatrix}
  1-p & p \\
  p & 1-p
\end{pmatrix}
$$

直列につないだら、それぞれの通信路行列の行列の積を計算したのが全体の通信路行列。

通信路容量は $1 - \mathcal{H}(p)$ で、$\mathcal{H}(p)$ はエントロピー関数。

$$
\mathcal{H}(p) = -p\log_2 p - (1-p)\log_2 (1-p)
$$

$0\log_2 0$ は0らしい。$p=1/2$ のとき最小。

TODO: 導出（教科書例題7.1）

### 加法的2元通信路

誤り源を $S_E$ とおくと、通信路容量は

$$
1-H(S_E)
$$

## 通信路符号化定理

通信路の通信路容量を $C$ とすると、$R\lt C$ であれば、任意の $\epsilon \gt 0$ に対し復号誤り率 $P_e$ が $P_e \lt \epsilon$ を満たすような情報速度 $R$ の符号が存在する。
しかし、$R\gt C$ ならば存在しない。すなわち、通信路容量はその通信路を介して任意に小さい誤り率で送ることができる情報速度の上限となるということである。

# 通信路符号化法

## 検査ビット

誤り検出や訂正のために余分に付加されるビット。以前巡回符号の定義にも書いたが、巡回符号の検査ビット長は最大次数と同じ。
「長さ $n$ の符号語が $m$ 次の生成多項式 $G(x)$ で生成される時、$(n, n-m)$ 符号となる」と覚えておくとよさそう。

## 組織符号

情報ビットに検査ビットを付け加えて並べた形式の符号。

## 最小距離

定義は以下。

$$
d_{\min} = \min_{c\neq c'} d(c,c')
$$

つまり、符号に含まれるあらゆる符号語のペアの距離の最小値。ハミング符号の最小距離は符号長によらず $3$ らしい。

線形符号では最小距離と最小ハミング重みが一致する（教科書の定理8.5）。

## 復号誤り率

ハミング符号では、訂正可能なビットが1だから、単一誤りの訂正（復号）ができる。しかし、二重誤り以上のものは訂正できない。
したがって、

$$
1-p(\text{誤りが発生しない})-p(\text{単一誤りが発生する})
$$

## 巡回符号

巡回符号は線形符号の一種で、符号化、復号、誤り検出と訂正に強いらしい。

### 定義

> 最大次数 $m (m > 0)$ で定数項が $1$ の任意の多項式を1つ選び、これを $G(x)$ とする。長さ $n$ (ただし $n > m$) の
> すべての2元系列に対応する $2^n$ 通りの多項式のうち、$G(x)$ で割り切れる多項式だけをすべて取り出し、それらを符号語とした
> 符号のことを巡回符号とよぶ。検査ビット長は $m$、情報ビット長は $n - m$ となる。このとき用いた $G(x)$ を生成多項式とよぶ。

ということは、語 $W(x)$ について、これが長さ $n$ 以下で $W(x) = Q(x)G(x)$ となるような $Q(x)$ が存在した場合、
$W(x)$ は $G(x)$ が生成する符号の符号語ということ。

:arrow_right: 2019

## 系統符号化

与えられた情報ビット列の多項式 $V(x)$ を $x^m$ 倍し、これを $G(x)$ で割った余りの多項式を検査ビットとすれば、巡回符号の符号語を作れる。
つまり、$Q(x)$ を $V(x)$ を $G(x)$ で割ったときの商、$C(x)$ を余りとすると、

$$
Q(x)G(x) = V(x)x^m + C(x)
$$

が成り立ち、符号語はこれになる。

## 最大訂正ビット数

誤り訂正能力は

$$
\left\lfloor \frac{d_{\min} - 1}{2} \right\rfloor
$$

と定義される。$d_{\min}$ は最小距離。ハミング符号だと $3$ を代入して $1$。

## 拡大ハミング符号

TODO

符号長を $2^m -1$ から増やすと、大体最小距離は小さくなる。

## 原始多項式

mod2の多項式において、$m$ 次多項式の周期の最大値は $2^m-1$ であり、この周期をもつ多項式のこと。
周期が符号長以上である場合、$d_{min}\ge 3$ らしい。符号長と $2^m-1$ が一致している場合はかなり怪しい。

## 周期

ある生成多項式 $G(x)$ が与えられたときに、$x^n - 1$ という形の多項式が $G(x)$ で割り切れるかどうかを調べ、これが割り切れるような最小の $n$ を、多項式 $G(x)$ の周期という。

## バースト誤り

通信路上での伝送において、密集して生じる誤りのこと。

### 巡回符号でのバースト誤りの検出と訂正

生成多項式 $G(x)$ の次数 $m$ 以下の長さの多重誤りは検出可能である。

多重誤りパターンの多項式表現を $E(x)$ とおくと、誤った受信語は $W(x) + E(x)$ とおけて、これが $G(x)$ で割り切れなければ、誤っていることがわかる。$W(x)$ は $G(x)$ で割り切れるので、
$E(x)$ が $G(x)$ で割り切れなければよい。誤りの長さが $m$ 以下であれば、$E(x)$ は割り切れないことがわかる[^1]。
$m+1$ の長さで検出できない多重誤りの例として、多重誤りパターン $E(x)$ が $G(x)$ 自身のときが挙げられる。

# 情報源符号化

## 瞬時符号

前から順番に符号語列を参照すれば、各記号の符号語の境目で復号すべき情報源記号が確定できる符号。

## 拡大情報源

Sの2次の拡大情報源と言われると、情報源記号が0,1であったとき、00、01、10、11をそれぞれまとめて一つの記号として扱う

### ブロックハフマン符号

拡大情報源についてハフマン符号化を行ってできる符号。平均符号長を求める時、記号列の長さで割ることを忘れないように！

## コンパクト符号

与えられた定常情報源 $S$ に対し、情報源記号ごとに符号語を割り当てる一意複合可能な符号のうち、その**平均符号長が一番短くなる**ような符号。

## クラフトの不等式

長さが $l_1,l_2,\dots ,l_M$ となる $M$ 個の符号語を持つ $r$ 元符号で、瞬時符号となるものが存在する必要十分条件。

$$
\frac{1}{r^{l_1}} + \frac{1}{r^{l_2}} + \cdots + \frac{1}{r^{l_M}} \le 1
$$

## 情報源符号化定理

情報源 $S$ に任意の一意復元可能な $r$ 元符号化をしたとき、平均符号長 $L$ が以下を満たす。

$$
L \ge \frac{H(S)}{\log_2 r}
$$

[^1]: これは実際に多項式表現を作ってみると、割り切れるかどうかチェックすべき部分の次数が $m$ より小さくなることがわかる。
